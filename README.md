# Chibi-LM

一个使用 PyTorch 从零实现的迷你语言模型学习项目。

## 项目简介

这是一个使用 Claude 创建的学习项目，目的是理解和实现 Transformer Decoder-only 架构的语言模型。项目实现了一个小型的语言模型，用于学习简单的文本序列模式。

## 功能特性

- **Transformer 架构**: 实现了 Decoder-only Transformer 模型
- **位置编码**: 正弦余弦位置编码
- **多头注意力机制**: 支持多头自注意力
- **因果掩码**: 确保自回归生成
- **训练流程**: 完整的训练、评估和文本生成流程

## 项目结构

```
.
├── model.py          # Transformer 模型实现
├── train.py          # 训练脚本
├── dl.py            # 数据加载器
├── gen-data.py      # 数据生成脚本
├── show_dataset.py  # 数据集查看工具
├── data/            # 训练数据和词汇表
└── checkpoints/     # 模型检查点
```

## 快速开始

### 训练模型

```bash
python train.py
```

### 查看数据集

```bash
python show_dataset.py
```

## 模型配置

- **词汇量**: 24 个词
- **模型维度**: 32
- **注意力头数**: 2
- **层数**: 2
- **前馈网络维度**: 64
- **最大序列长度**: 16

## 学习目标

通过此项目学习：

1. Transformer 架构的基本原理
2. 自注意力机制的实现
3. 语言模型的训练流程
4. 文本生成的基本方法
